services:
  ollama:
    image: ollama/ollama:latest
    container_name: resume-parser-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./ollama-init.sh:/ollama-init.sh:ro
    environment:
      - OLLAMA_MODEL=gemma3:4b
    entrypoint: ["/bin/bash", "/ollama-init.sh"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/bin/ollama", "list"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s
    networks:
      - resume-parser-network

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: resume-parser-backend
    ports:
      - "8000:8000"
    environment:
      - DEBUG=True
      - LOG_LEVEL=INFO
      - API_PREFIX=/api/v1
      - LLM_BASE_URL=http://ollama:11434
      - LLM_MODEL_NAME=gemma3:4b
      - LLM_TIMEOUT=60
    volumes:
      - ./backend:/app
      - /app/.venv
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - resume-parser-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: resume-parser-frontend
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://localhost:8000/api/v1
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - resume-parser-network

volumes:
  ollama-data:
    driver: local

networks:
  resume-parser-network:
    driver: bridge
